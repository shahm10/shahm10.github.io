---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
layout: single
---

<!-- 
I’m **Seung Hyun (Sally) Hahm**, a Computer Science M.S. student at **Dartmouth College**, where I study how AI can perceive and communicate the world for those who cannot see it.  


My research explores how multimodal AI systems can **see, understand, and communicate** what’s happening in videos. I develop models that analyze visual scenes, recognize relationships and emotions between people, and transform these insights into natural **audio descriptions**.
These systems aim not only to describe visual content but also to **convey narrative and emotional meaning**, helping visually impaired users experience stories more fully.

During my undergraduate studies at Dartmouth, I completed a **High Honors Thesis** under **Professor SouYoung Jin** on **character relationship prediction in movies**, which led to my first publication submission:  
> *Character Relationship Prediction in Movies: Toward Emotionally-Aware Automatic Audio Descriptions* (WACV 2026, under review)

I have received **academic citations** in *Computer Vision, Deep Learning,* and *Multimodal Generative AI*.

Currently, I am also developing **retrieval-informed video description systems** that enhance how AI narrates complex scenes. (Details of this project are under review and will be publicly released following publication.)

Previously, I co-founded **NextCare** (Singapore), designing privacy-preserving health data exchange and AI assistants.


Teaching
======

At Dartmouth, I have served as:

**Graduate Teaching Assistant — COSC 76: Artificial Intelligence**  
*Fall 2025 · Prof. Soroush Vosoughi*  
- Assisted in designing and grading assignments focused on search algorithms, probabilistic reasoning, and reinforcement learning.  

**Teaching Assistant — COSC 74: Machine Learning**  
*Spring 2025 · Prof. Soroush Vosoughi*  
- Supported course instruction on supervised and unsupervised learning, neural networks, and evaluation techniques.  

**Class Notes Ally — COSC 89.32: Multimodal Generative AI**  
*Fall 2024 · Prof. Yu-Wing Tai · Student Accessibility Services*  
- Collaborated with Dartmouth’s Student Accessibility Services to provide accessible lecture notes.  
- Helped adapt class materials to be more accessible through clear textual descriptions and structured note formats.


Long-Term Vision
======
My research connects **video understanding**, **language grounding**, and **assistive technology**.  
I aim to develop AI systems that can *perceive, describe, and interact*—extending from accessible narration to embodied intelligence capable of supporting people with disabilities.
 -->



I’m Seung Hyun (Sally) Hahm, an M.S. student in Computer Science at Dartmouth College advised by Professor [SouYoung Jin](https://souyoungjin.github.io), where I study how AI can perceive and communicate the world for those who cannot see it.

**What is red?** (255, 0, 0)? A wavelength range? The color of a rose?

A friend once told me he was colorblind, and I tried—earnestly—to explain what “red” meant. I had facts and metaphors, but I couldn’t translate the experience itself. That gap stuck with me. It showed me how easily systems feel complete only because they quietly assume a “default” sensory baseline—quickly excluding others who do not share it.

Most vision systems assume the user can see the rest. For blind and low-vision (BLV) users, the job is different: translate visual information into another channel without losing what makes it meaningful—who someone is, what changed, and why it matters. 

I am primarily motivated by this question: **how can computer vision become an independent pathway for understanding, not an add-on to sight?**


---
### Research
My research focuses on how computer vision can communicate the world to people who cannot rely on sight. 

- **Narrative-grounded Audio Description for movies**: Generated long-form video narration that preserves continuity across scenes—who someone is, what changed, and why it matters.

- **Retrieval-Informed Video Understanding (RAG)**: Grounding descriptions with structured external context to reduce hallucination and improve story fidelity.

- **Social & Emotional Understanding in Movies**: Modeled inter-character relationships to enrich descriptions with meaningful social dynamics.

- **Movie AD Evaluation Framework**: Built comprehension-based evaluation with a 6,000+ Q&A benchmark to test whether descriptions actually support BLV understanding.

### Publications

- Tell the Story, Not the Frames: Narrative-Aware Retrieval for Audio Description — CVPR 2026 (under review)

- Character Relationship Prediction in Movies: Toward Emotionally-Aware Automatic Audio Descriptions — High Honors Thesis

---

### Projects

**Friendly Spot — Socially Adaptive Human–Robot Interaction (Boston Dynamics Spot), Dartmouth College**

* Built an affect-aware proxemics system that fuses emotion, pose, gesture, identity, and distance into a scalar comfort score, then selects 1 of 6 socially meaningful behaviors (e.g., approach slowly, keep distance, back away, sit).
* Validated key perception modules in controlled indoor trials: 90% gesture recognition (36/40 hands), >80% emotion alignment with prompted expressions, and <5% repeat-recognition errors.

**InstructBLIP Video-Captioning Optimization, Dartmouth College (Sep 2024–Dec 2024)**

* Optimized InstructBLIP’s Q-Former for video–language modeling on MSR-VTT improving video comprehension with a +24 CIDEr gain.
* Achieved #2 on the MSR-VTT leaderboard using only 6K video–text pairs.

### Academic Recognition
- Graduated with High Honors.  
- Received academic citations for exceptional academic performance in *Computer Vision*, *Deep Learning*, and *Multimodal Generative AI*.

---

### Work Experience
- **NextCare** — Co-founded a health-data startup building blockchain-based infrastructure for privacy-preserving medical-record exchange and AI-driven health assistants.  
  Focused on designing scalable systems and applying AI for accessible, trustworthy healthcare solutions.

---

### Teaching
- **Graduate TA — COSC 76: Artificial Intelligence** *(Fall 2025, Prof. Soroush Vosoughi)*  
- **Undergraduate TA — COSC 74: Machine Learning** *(Spring 2025, Prof. Soroush Vosoughi)*  
- **Class Notes Ally — COSC 89.32: Multimodal Generative AI** *(Fall 2024, Prof. Yu-Wing Tai)*  
  – A position through Dartmouth’s **Student Accessibility Services**, providing course notes and adapted materials for students with documented disabilities to ensure equitable academic access.

---
