---
permalink: /about/
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---



I’m **Seung Hyun (Sally) Hahm**, a Computer Science M.S. student at **Dartmouth College**, where I study how AI can perceive and communicate the world for those who cannot see it.  


My research explores how multimodal AI systems can **see, understand, and communicate** what’s happening in videos. I develop models that analyze visual scenes, recognize relationships and emotions between people, and transform these insights into natural **audio descriptions**.
These systems aim not only to describe visual content but also to **convey narrative and emotional meaning**, helping visually impaired users experience stories more fully.

During my undergraduate studies at Dartmouth, I completed a **High Honors Thesis** under **Professor SouYoung Jin** on **character relationship modeling in movies**, which led to my first publication submission:  
> *Character Relationship Prediction in Movies: Toward Emotionally-Aware Automatic Audio Descriptions* (WACV 2026, under review)

I graduated **with High Honors** and received **academic citations** in *Computer Vision, Deep Learning,* and *Multimodal Generative AI*.

Previously, I co-founded **NextCare** (Singapore), designing privacy-preserving health data exchange (blockchain + SSI) and AI assistants.

Long-term, along with video understanding, I’m interested in extending my work toward **embodied AI** — intelligent agents that integrate visual understanding, language grounding, and memory to interact naturally in the physical world.


I am also developing **retrieval-informed video description systems** that enhance how AI narrates complex scenes.  

(Details of this project are under review and will be publicly released following publication.)
Teaching
======

At Dartmouth, I have served as:
- **Graduate TA**, *COSC 76: Artificial Intelligence* (Fall 2025, Prof. Soroush Vosoughi)  
- **TA**, *COSC 74: Machine Learning* (Spring 2025, Prof. Soroush Vosoughi)  
- **Class Notes Ally**, *COSC 89.32: Mutlimodal Generative AI* (Fall 2024, Prof. Yu-Wing Tai, Student Accessibility Services)


**Graduate Teaching Assistant — COSC 76: Artificial Intelligence**  
*Fall 2025 · Prof. Soroush Vosoughi*  
- Assisted in designing and grading assignments focused on search algorithms, probabilistic reasoning, and reinforcement learning.  

**Teaching Assistant — COSC 74: Machine Learning**  
*Spring 2025 · Prof. Soroush Vosoughi*  
- Supported course instruction on supervised and unsupervised learning, neural networks, and evaluation techniques.  
- Guided students through implementation projects using Python, PyTorch, and scikit-learn.  
- Provided office-hour support for conceptual understanding and debugging model training pipelines.

**Class Notes Ally — COSC 89.32: Multimodal Generative AI**  
*Fall 2024 · Prof. Yu-Wing Tai · Student Accessibility Services*  
- Collaborated with Dartmouth’s Student Accessibility Services to provide accessible lecture notes.  
- Helped adapt class materials to be more accessible through clear textual descriptions and structured note formats.






Long-Term Vision
======
My research connects **video understanding**, **language grounding**, and **assistive technology**.  
I aim to develop AI systems that can *perceive, describe, and interact*—extending from accessible narration to embodied intelligence capable of supporting people with disabilities.

