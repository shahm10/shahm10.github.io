---
title: "Research"
permalink: /research/
layout: collection
collection: publications
entries_layout: list
---

### Research Overview
My research centers on **multimodal video understanding** and **audio description generation**, with the broader goal of building AI systems that help the visually impaired experience visual media through language.  

I study how machines can perceive social and emotional cues in video and translate them into **contextually rich, emotionally grounded audio descriptions**. My work combines **vision-language modeling**, **transformer-based reasoning**, and **relationship inference** to enable AI systems to go beyond literal captioning — toward storytelling that captures human intent, affect, and context.

---

### Character Relationship Module (CRM)
At Dartmouth’s **Video Understanding Lab**, I developed a **Character Relationship Module** that enables video captioning models to interpret **social dynamics** within scenes — identifying who interacts, how they feel, and why those interactions matter.  

The model integrates **inter-character sentiment classification** directly into the captioning pipeline, allowing generated audio descriptions to reflect not just actions but emotional context.  
It achieved **68.3% accuracy** in relationship prediction on a large-scale benchmark dataset and forms the foundation of my paper:  

> *Hahm, S. H. & Jin, S. Y. “Character Relationship Prediction in Movies: Toward Emotionally-Aware Automatic Audio Descriptions.” WACV 2026 (under review)*  

To support this work, I built a benchmark dataset of **669K character-sentiment annotations** across 202 movies using an automated **LLaMA-3–based data generation pipeline**, enabling large-scale weak supervision for social understanding in video.

---

### Context-Grounded Audio Description Models
I am also developing **retrieval-informed video description systems** that enhance how AI narrates complex scenes.  

(Details of this project are under review and will be publicly released following publication.)

---

### Broader Interests
- Multimodal learning for **video understanding and narrative generation**  
- **Audio description** and accessibility-oriented applications of AI  
- Integrating **memory and grounding** in long-horizon visual reasoning  
- Extending perceptual models toward **interactive, embodied AI** that can perceive, reason, and communicate naturally with humans
